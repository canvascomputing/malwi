#!/bin/bash

# Tiny DistilBERT Model Training Script
# This script trains a tiny version of DistilBERT for resource-constrained environments

set -e  # Exit on any error

echo "ü§ñ Starting Tiny DistilBERT model training..."
echo "   This creates a much smaller model suitable for edge devices"
echo

# Check if processed data exists
if [ ! -f "benign_processed.csv" ] || [ ! -f "malicious_processed.csv" ]; then
    echo "‚ùå Error: Processed data files not found"
    echo "   Please run preprocess_data.sh first to generate processed data"
    exit 1
fi

echo "‚úÖ Processed data files found"

# Check if tokenizer exists
if [ ! -f "malwi_models/tokenizer.json" ]; then
    echo "‚ùå Error: No tokenizer found at malwi_models/"
    echo "   Please run train_tokenizer.sh first to create the tokenizer"
    exit 1
fi

echo "‚úÖ Tokenizer found at malwi_models/"
echo

# Define vocabulary size (should match tokenizer training)
VOCAB_SIZE=5000

# Train Tiny DistilBERT model
echo "üöÄ Training Tiny DistilBERT model..."
echo "   ‚Ä¢ Loading pre-trained tokenizer from malwi_models/"
echo "   ‚Ä¢ Training data: benign_processed.csv, malicious_processed.csv"
echo "   ‚Ä¢ Vocabulary size: $VOCAB_SIZE"
echo "   ‚Ä¢ Model size: TINY (256 hidden dimensions)"
echo "   ‚Ä¢ Epochs: 5 (more epochs for smaller model)"
echo "   ‚Ä¢ Using 1 processor for training"
echo

uv run python -m src.research.train_distilbert \
    -b benign_processed.csv \
    -m malicious_processed.csv \
    --epochs 5 \
    --num-proc 1 \
    --vocab-size $VOCAB_SIZE \
    --model-size tiny \
    --model-output-path malwi_models_tiny

echo
echo "üéâ Tiny DistilBERT model training completed!"
echo
echo "üìã Model files saved to malwi_models_tiny/:"
echo "   ‚Ä¢ Trained Tiny DistilBERT model weights and config"
echo "   ‚Ä¢ Training metrics and logs"
echo "   ‚Ä¢ Pre-existing tokenizer (preserved)"
echo
echo "üí° Tiny Model Specifications:"
echo "   ‚Ä¢ Hidden dimensions: 256 (vs 768 standard)"
echo "   ‚Ä¢ Attention heads: 4 (vs 12 standard)"
echo "   ‚Ä¢ Layers: 4 (vs 6 standard)"
echo "   ‚Ä¢ Vocabulary: $VOCAB_SIZE tokens"
echo "   ‚Ä¢ Approximate size: ~35MB (vs ~210MB small, ~250MB standard)"
echo "   ‚Ä¢ Parameters: ~5.5M (vs ~66M standard)"
echo
echo "‚ö° Performance Trade-offs:"
echo "   ‚Ä¢ Much faster inference (4-5x faster)"
echo "   ‚Ä¢ Lower memory usage (~85% reduction)"
echo "   ‚Ä¢ Slightly lower accuracy (expect ~2-3% drop)"
echo "   ‚Ä¢ Ideal for edge devices or high-throughput scenarios"
echo