#!/bin/bash

# Tokenizer Training Script
# This script trains a custom tokenizer for machine learning models

set -e  # Exit on any error

echo "üî§ Starting tokenizer training..."
echo

# Define vocabulary size (should match tokenizer training)
VOCAB_SIZE=30522

# Check if processed data exists
if [ ! -f "benign_processed.csv" ] || [ ! -f "malicious_processed.csv" ]; then
    echo "‚ùå Error: Processed data files not found"
    echo "   Please run preprocess_data.sh first to generate processed data"
    exit 1
fi

echo "‚úÖ Processed data files found"
echo

# Train custom tokenizer
echo "üöÄ Training custom tokenizer..."
echo "   ‚Ä¢ Using 438 base tokens from function mapping + top frequent tokens from data"
echo "   ‚Ä¢ Training on: benign_processed.csv, malicious_processed.csv"
echo "   ‚Ä¢ Total tokens: 5000 (438 base + 4562 data-derived)"
echo "   ‚Ä¢ Output directory: malwi_models/"
echo "   ‚Ä¢ Saving computed tokens for inspection"
echo

uv run python -m src.research.train_tokenizer \
    -b benign_processed.csv \
    -m malicious_processed.csv \
    -o malwi_models \
    --top-n-tokens $VOCAB_SIZE \
    --save-computed-tokens \
    --force-retrain

echo
echo "üéâ Tokenizer training completed successfully!"
echo
echo "üìã Generated files in malwi_models/:"
echo "   ‚Ä¢ tokenizer.json - Main tokenizer configuration"
echo "   ‚Ä¢ tokenizer_config.json - Tokenizer metadata"
echo "   ‚Ä¢ vocab.json - Vocabulary mapping"
echo "   ‚Ä¢ merges.txt - BPE merge rules"
echo "   ‚Ä¢ computed_special_tokens.txt - All special tokens (base + data)"
echo "   ‚Ä¢ base_tokens_from_function_mapping.txt - Base tokens only"
echo
echo "üìñ Next steps:"
echo "   ‚Ä¢ Review computed_special_tokens.txt if needed"
echo "   ‚Ä¢ Run train_distilbert.sh to train the DistilBERT model"
echo "   ‚Ä¢ The tokenizer will be automatically loaded from malwi_models/"
echo